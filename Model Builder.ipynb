{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numer albumu: 123641\n",
    "# Systemy rekomendacyjne - projekt\n",
    "### Temat: Mikrousługa budowy modelu predykcyjnego klasy ML (typu Wide&Deep) z użyciem biblioteki TensorFlow (Model Builder)\n",
    "\n",
    "---\n",
    "\n",
    "### Czym jest model Wide&Deep?\n",
    "\n",
    "__*Model Wide&Deep*__ to model, który ma na celu połączenie liniowego modelu oraz głębokiej sieci nauronowej. Często jest używany dla systemów rekomendacyjnych, czy problemów związanych z wyszukiwaniem. Część Wide to liniowy model, którego celem jest zapamiętywanie najlepszych korelacji dla zapytania (np. użytkownicy lubią gatunki filmów horror i komedia, dlatego będą wyszukiwane dla nich filmy z tych gatnków). Deep to gęboka sieć neuronowa, której celem jest odnajdywanie w przestrzeni embedding wektorów leżących blisko siebie, aby wprowadzić element zaskoczenia dla użytkownika (np. to, że użytkownicy lubią horrory i komedie, nie oznacza, że nie obejrzą dokumentu).\n",
    "\n",
    "---\n",
    "\n",
    "### Analiza plików **.dat* hetrec2011-movielens-2k Dataset:\n",
    "\n",
    "Na początku warto zapoznać się z jakimi danymi mamy do czynienia. W tym celu przejrzano wszystkie pliki i zrobiono ich podsumowanie:\n",
    "\n",
    "1. __*movies.dat*__:\n",
    "    * __*id*__ - identyfikator filmu,\n",
    "    * __*title*__ - tytuł filmu,\n",
    "    * __*imdbID*__ - identyfikator filmu w serwisie IMDB,\n",
    "    * __*spanishTitle*__ - tytuł filmu po hiszpańsku,\n",
    "    * __*imdbPictureURL*__ - URL do okładki filmu w serwisie IMDB,\n",
    "    * __*year*__ - rok wydania filmu,\n",
    "    * __*rtID*__ - identyfikator filmu w serwisie Rotten Tomatoes,\n",
    "    * __*rtAllCriticsRating*__ - całkowita ocena filmu przez wszystkich krytyków w serwisie Rotten Tomatoes,\n",
    "    * __*rtAllCriticsNumReviews*__ - ilość recenzji krytyków,\n",
    "    * __*rtAllCriticsNumFresh*__ - ilość pozytywnych ocen (powyżej 60%) przez wszystkich krytyków,\n",
    "    * __*rtAllCriticsNumRotten*__ - ilosc negatywnych ocen (pniżej 60%) przez wszystkich krytyków,\n",
    "    * __*rtAllCriticsScore*__ - procentowy wynik filmu oceniany przez wszystkich krytyków,\n",
    "    * __*rtTopCriticsRating*__ - całkowita ocena filmu przez najlepszych krytyków,\n",
    "    * __*rtTopCriticsNumReviews*__ - ilość recenzji najlepszych krytyków,\n",
    "    * __*rtTopCriticsNumFresh*__ - ilość pozytywnych recenzji najlepszych krytyków,\n",
    "    * __*rtTopCriticsNumRotten*__ - ilość negatywnych recenzji najlepszych krytyków,\n",
    "    * __*rtTopCriticsScore*__ - procentowa ocena filmu przez najlepszych krytyków,\n",
    "    * __*rtAudienceRating*__ - całkowita ocena filmu przez wszystkich głosujących użytkowników,\n",
    "    * __*rtAudienceNumRatings*__ - ilość użytkowników, która głosowała,\n",
    "    * __*rtAudienceScore*__ - procentowa ocena filmu przez wszystkich głosujących użytkowników,\n",
    "    * __*rtPictureURL*__ - URL do okładki filmu w serwisie Rotten Tomatoes.\n",
    "    \n",
    "2. __*movie_genres.dat*__:\n",
    "    * __*movieID*__ - identyfikator filmu,\n",
    "    * __*genre*__ - gatunek filmu. \n",
    "    \n",
    "3. __*movie_directors.dat*__:\n",
    "    * __*movieID*__ - identyfikator filmu,\n",
    "    * __*directorID*__ - identyfikator reżysera filmu,\n",
    "    * __*directorName*__ - imię i nazwisko reżysera filmu. \n",
    "    \n",
    "4. __*movie_actors.dat*__:\n",
    "    * __*movieID*__ - identyfikator filmu,\n",
    "    * __*actorID*__ - identyfikator aktora występującego w filmie,\n",
    "    * __*actorName*__ - imię i nazwisko aktora,\n",
    "    * __*ranking*__ - miejsce atora w rankingu.    \n",
    "\n",
    "5. __*movie_countries.dat*__:\n",
    "    * __*movieID*__ - identyfikator filmu,\n",
    "    * __*country*__ - kraj produkcji filmu.\n",
    "\n",
    "6. __*movie_locations.dat*__:\n",
    "    * __*movieID*__ - identyfikator filmu,\n",
    "    * __*location1*__ - kraj, w którym nagrywano film,\n",
    "    * __*location2*__ - region, w którym nagrywano film,\n",
    "    * __*location3*__ - miasto, w którym nagrywano film,\n",
    "    * __*location4*__ - ulica/miejsce, w którym nagrywano film.\n",
    "    \n",
    "7. __*tags.dat*__:\n",
    "    * __*id*__ - identyfikator tagu,\n",
    "    * __*value*__ - tag.\n",
    "\n",
    "8. __*movie_tags.dat*__:\n",
    "    * __*movieID*__ - identyfikator filmu,\n",
    "    * __*tagID*__ - identyfikator tagu,\n",
    "    * __*tagWeight*__ - waga tagu.\n",
    "    \n",
    "9. __*user-taggedmovies-timestamps.dat*__:\n",
    "    * __*userID*__ - identyfikator użytkownika,\n",
    "    * __*movieID*__ - identyfikator filmu,\n",
    "    * __*tagID*__ - identyfikator tagu,\n",
    "    * __*timestamp*__ - sygnatura czasowa użycia tagu przez użytkownika w kontekście filmu.    \n",
    "    \n",
    "10. __*user-taggedmovies.dat*__:\n",
    "    * __*userID*__ - identyfikator użytkownika,\n",
    "    * __*movieID*__ - identyfikator filmu,\n",
    "    * __*tagID*__ - identyfikator tagu,\n",
    "    * __*date_day*__ - dzień użycia tagu,\n",
    "    * __*date_month*__ - miesiąc użycia tagu,\n",
    "    * __*date_year*__ - rok użycia tagu,\n",
    "    * __*date_hour*__ - godzina użycia tagu,\n",
    "    * __*date_minute*__ - minut użycia tagu,\n",
    "    * __*date_second*__ - sekunda użycia tagu.\n",
    "    \n",
    "11. __*user-ratedmovies-timestamps.dat*__:\n",
    "    * __*userID*__ - identyfikator użytkownika,\n",
    "    * __*movieID*__ - identyfikator filmu,\n",
    "    * __*rating*__ - ocena filmu przez użytkownika,\n",
    "    * __*timestamp*__ - sygnatura czasowa ocenienia filmu przez użytkownika.\n",
    "    \n",
    "12. __*user-taggedmovies.dat*__:\n",
    "    * __*userID*__ - identyfikator użytkownika,\n",
    "    * __*movieID*__ - identyfikator filmu,\n",
    "    * __*rating*__ - ocena filmu przez użytkownika,\n",
    "    * __*date_day*__ - dzień ocenienia filmu,\n",
    "    * __*date_month*__ - miesiąc ocenienia filmu,\n",
    "    * __*date_year*__ - rok ocenienia filmu,\n",
    "    * __*date_hour*__ - godzina ocenienia filmu,\n",
    "    * __*date_minute*__ - minut ocenienia filmu,\n",
    "    * __*date_second*__ - sekunda ocenienia filmu.\n",
    "\n",
    "---\n",
    "\n",
    "### Budowanie modelu\n",
    "\n",
    "Mimo tak wielu danych do zbudowania modelu predykcyjnego użyto tylko pliku __*user-ratedmovies-timestamps.dat*__, z którego wybrano kolumny userID, movieID oraz rating. Po części wynika to z ograniczonej mocy obliczeniowej, a po części z założenia, które przyjęto - do budowy dobrego modelu wcale nie trzeba mieć dużej ilości danych, czasami mniej oznacza więcej.\n",
    "\n",
    "__UWAGA! Wszelkie ścieżki do plików należy dostosować do swoich folderów na dysku!__\n",
    "\n",
    "---\n",
    "#### Wczytanie danych\n",
    "\n",
    "Pierwszym krokiem jest zaimportowanie wszystkich potrzebnych bibliotek oraz wczytanie danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>movieID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>32</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>110</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>160</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>163</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  movieID  rating\n",
       "0      75        3     1.0\n",
       "1      75       32     4.5\n",
       "2      75      110     4.0\n",
       "3      75      160     2.0\n",
       "4      75      163     4.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#ścieżka do zbioru danych hetrec2011\n",
    "path_to_file = 'F:/Ania/hetrec2011-movielens-2k-v2/'\n",
    "\n",
    "#wczytywanie pliku user-ratedmovies-timestamps.dat\n",
    "ratings = pd.read_csv(path_to_file + 'user_ratedmovies-timestamps.dat', engine = 'python', delimiter=\"\\t\", header = 0, usecols = [0, 1, 2])\n",
    "\n",
    "#wypisanie początku wczytanych danych\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Użyte argumenty dla funkcji __*read_csv()*__ to:\n",
    "1. __engine__ - mechanizm parsera, wybrałam typ __*'python'*__ ze względu na większą funkcjonalność\n",
    "2. __delimiter__ - separator, w tym przypadku jest to tabulator - oznaczony poprzez __*\\t*__\n",
    "3. __header__ - wartość __*0*__ oznacza wnioskowanie nazw kolumn na podstawie pierwszego wiersza, argument użyty, aby nie wczytywać nazw kolumn jako pierwszy wiersz\n",
    "4. __usecols__ - użycie tego argumentu poprawia szybkośc wczytywania i zużycie pamięci, tutaj wczytywane są pierwsze 3 kolumny - userID, movieID oraz rating dlatego ten argument ma wartość __*[0,1,2]*__\n",
    "\n",
    "Następnie sprawdzono ilość danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(855598, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sprawdzenie ilości danych\n",
    "ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ze względu na to, że w zbiorach mogą zdażyć się duplikaty usunięto je funkcją __*drop_duplicates()*__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(855598, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#usuwanie duplikatów z kolumn\n",
    "ratings.drop_duplicates()\n",
    "\n",
    "#sprawdzenie ilości danych po usunięciu duplikatów\n",
    "ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po sprawdzeniu ilości danych, można zauważyć, że wczytane kolumny nie zawierały duplikatów.\n",
    "___\n",
    "\n",
    "#### Budowanie feature columns\n",
    "\n",
    "Najpierw wyznaczono unikalne wartości dla kolumny userID oraz movieID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users:  2113 , movies:  10109\n"
     ]
    }
   ],
   "source": [
    "#unikalne wartości dla kolumn users i movies\n",
    "users = list(pd.unique(ratings['userID']))\n",
    "movies = list(pd.unique(ratings['movieID']))\n",
    "\n",
    "#wypisanie ilości unikalnych wartości\n",
    "print(\"users: \", len(users), \", movies: \", len(movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwsze budowane feature columns należą do kategorii __*categorical_column_with_vocabulary_list*__. Ten rodzaj charakteryzuje się reprezentacją ciągów znaków jako one-hot wektor. Wybrany rodzaj mapuje każdy string na liczbę całkowitą na podstawie listy słów. W tym przypadku listą taką są unikatowe wartości __*users*__ oraz __*movies*__. Te features są potrzebne do zbudowania kolejnych i nie będą uczestniczyły bezpośrednio w modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pierwszy rodzaj features - categorical_column_with_vocabulary_list\n",
    "user_id_vocabulary = tf.feature_column.categorical_column_with_vocabulary_list('userID', users)\n",
    "movie_id_vocabulary = tf.feature_column.categorical_column_with_vocabulary_list('movieID', movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolejnym rodzajem, na który się zdecydowano jest __*indicator_column*__. Nie działają one bezpośrednio na danych, ale na feature columns z rodzaju __*categorical_column*__. Również wykorzystują one-hot wektor - każda kategoria jest traktowana jako element w jednym one-hot wektor, gdzie dopasowana kategoria ma wartość 1, a reszta 0. Te features znajdą się bezpośrednio w modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drugi rodzaj features - indicator_column\n",
    "user_id_indicator_vocabulary = tf.feature_column.indicator_column(user_id_vocabulary)\n",
    "movie_id_indicator_vocabulary = tf.feature_column.indicator_column(movie_id_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trzecim rodzajem fature columns są __*bucketized_column*__. Dzięki temu rodzajowi, można podzielić wartości na różne kategorie o wartościach liczbowych. Dzięki temu tworzony jest wektor z weloma wagami, a nie tylko jedną. Model może na tym zyskać dużą elastyczność. W tym wypadku podzielono dane na 3 zbiory (argument __*boundaries*__). Przedziały zostały dobrane na podstawie unikalnych wartości (podzielenie unikatów przez 3). Ten rodzaj również nie bierze bezpośredniego udziału w modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_bucketized=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('userID'), boundaries=[700, 1400])\n",
    "movie_id_bucketized=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('movieID'), boundaries=[3370, 6740])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponownie użyto feature column z rodzaju __*indicator_column*__. Tym razem bazą były __*bucketized_column*__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_indicator_bucketized = tf.feature_column.indicator_column(user_id_bucketized)\n",
    "movie_id_indicator_bucketized = tf.feature_column.indicator_column(movie_id_bucketized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do zbudowania części __*Wide*__ modelu użyto feature column z rodzaju __*crossed_column*__. Dzięki temu model może nauczyć się oddzielnych wag dla features. Argument __*hash_bucket_size*__ musi być wartością całkowitą większą od 1. Oznacza on liczbę kubełków na jakie zostanie podzielona siatka stworzona z features. W tym wypadku jest to liczba 5000 również ze względu na liczbę unikatowych wartości - w przypadku userID ~2500*2 = 5000, a dla movieID ~10000/2 = 5000. Na użycie tej wartości nie ma żadnych dowodów, jest to twórczość autora projektu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_columns = [\n",
    "    tf.feature_column.crossed_column(['userID', 'movieID'], hash_bucket_size=5000)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Budowanie części __*Deep*__ jest bardziej złożone. Użyto tutaj aż czterech feature columns, wszystkich stworzonych wcześniej z rodzaju __*indicator_column*__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_columns = [\n",
    "    user_id_indicator_bucketized,\n",
    "    movie_id_indicator_bucketized,\n",
    "    user_id_indicator_vocabulary,\n",
    "    movie_id_indicator_vocabulary\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Model\n",
    "\n",
    "Zbudowany model będzie miał zapisywane parametry, grafy w folderze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder, w którym będzie zapisywany model\n",
    "model_directory = 'F:/Ania/model-builder-all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybór klasyfikatora padł na __*DNNLinearCombinedClassifier*__. Łączy on zarówno cechy __*Wide*__, jak i __*Deep*__. Parametry:\n",
    "1. __*model_dir*__ - katalog modelu, w którym będą zapisywane parametry, ale również checkpoint\n",
    "2. __*linear_feature_columns*__ - część Wide modelu, feature columns wykorzystywane przez liniową część klayfikatora\n",
    "3. __*dnn_feature_columns*__ - część Deep modelu, feature column wykorzystywane przez głęboką część klasyfikatora\n",
    "4. __*linear_optimizer*__ - optymalizator dla części liniowej. Wybrany został __*AdadeltaOptimizer*__ z parametrem __*learning_rate*__ = 0.1 (wskaźnik uczenia się). Optymalizator został wybrany ze względu na rzekomą dobrą współpracę z liniowymi modelami. Pierwotnym wyborem był __*AdamOptimizer*__ jednak okazał się on nietrafiony. __*AdadeltaOptimizer*__ podniósł ostateczny wynik o 0.01. \n",
    "5. __*dnn_optimizer*__ - optymalizator dla części głębokiej. Wybrany został __*AdagradOptimizer*__, który jest wartością domyślną. Parametr __*learning_rate*__ również został ustawiony na 0.1.\n",
    "1. __*dnn_hidden_units*__ - lista wartw ukrytych dla części głębokiej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'F:/Ania/model-builder-all', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000015EA9D4FB38>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "#zbudowany model\n",
    "model = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_directory,\n",
    "    linear_feature_columns=wide_columns,\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    linear_optimizer=tf.train.FtrlOptimizer(learning_rate=0.1),\n",
    "    dnn_optimizer=tf.train.AdagradOptimizer(learning_rate=0.1),\n",
    "    dnn_hidden_units=[512, 254]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Podział zbioru\n",
    "\n",
    "Zbiór danych hetrec zawiera tylko czyste dane, które nie są rozdzielone na zbiór treningowy i testowy. Wobec tego dostępne dane należy podzielić na takie zbiory.\n",
    "Pierwszym krokiem tutaj jest określenie wartości progowej. Według Rotten Tomatoes film jest uznawany za dobry w momencie kiedy użytkownik ocenił do na przynajmniej 60% czyli 3.5. Takie wartości zostaną wybrane ze wszystkich danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ustalona wartość dla \"dobrych\" filmów\n",
    "ratings[\"rating\"] = ratings[\"rating\"] >= 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie następuje właściwy podział dzięki metodzie __*train_test_split()*__. Ponieważ zbiór testowy powinien mieć wielkość około 10-20% zbioru treningowego wybrano 20% (argument __*test_size*__). Argument __*shuffle*__ został ustawiony na True co oznacza przemieszanie danych przed podziałem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#właściwy podział zbioru\n",
    "y = ratings[\"rating\"]\n",
    "X = ratings.drop(\"rating\",axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Trening i ewaluacja modelu\n",
    "\n",
    "Zarówno do treningu jak i ewaluacji należy przygotować specjalne funkcje - __*train_function()*__ oraz __*test_function()*__. Charakteryzują się tym, że dzięki nim obiekty z biblioteki __*pandas*__ - __*DataFrame*__ są bezpośrednio podawane do modelu. __*train_function()*__ jako argumenty dostanie wcześniej zdefiniowane __*X_train*__ oraz __*y_train*__, a __*test_function()*__ - __*X_test*__ oraz __*y_test*__. Różnią się one również zwracanym parametrem __*shuffle*__, którego zadaniem jest odczytywanie danych w randomowej kolejności dla zbioru treningowego oraz w ustalonej (od początku) dla zbioru testowego.\n",
    "\n",
    "Model jest trenowany za pomocą funkcji __*train()*__, która przyjmuje __*train_function()*__ jako argument. Dodatkowym argumentem jest __*steps*__, który jest liczbą całkowitą i oznacza ilość kroków trenowania modelu. Wybranych zostało __*2000*__ kroków, aby uniknąć przetrenowania modelu, ale jednocześnie uzyskać jak najlepszy wynik.\n",
    "__*UWAGA! Po pierwszym treningu i ewaluacji modelu należy zmienić folder dla modelu ze względu na wykreowany checkpoint. Sprawia to, że zamiast liczyć od początku, do porzedniego wyliczenia dochodzą nam kolejne kroki co może mieć wpływ na późniejsze przetrenowanie modelu!*__\n",
    "\n",
    "Do ewaluacji modelu użyto funckji __*evaluate()*__. Jako argument przyjmuje ona __*test_function()*__. Funckja zwraca wyniki modelu dla różnych miar ewaluacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into F:/Ania/model-builder-all\\model.ckpt.\n",
      "INFO:tensorflow:loss = 88.79872, step = 1\n",
      "INFO:tensorflow:global_step/sec: 22.6359\n",
      "INFO:tensorflow:loss = 81.041794, step = 101 (4.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.3942\n",
      "INFO:tensorflow:loss = 67.92705, step = 201 (4.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.3778\n",
      "INFO:tensorflow:loss = 76.07112, step = 301 (4.278 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.1351\n",
      "INFO:tensorflow:loss = 64.81767, step = 401 (4.323 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.1993\n",
      "INFO:tensorflow:loss = 74.889, step = 501 (4.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.2154\n",
      "INFO:tensorflow:loss = 67.70332, step = 601 (4.307 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.013\n",
      "INFO:tensorflow:loss = 64.83331, step = 701 (4.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.2531\n",
      "INFO:tensorflow:loss = 73.46153, step = 801 (4.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.2909\n",
      "INFO:tensorflow:loss = 63.46515, step = 901 (4.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.318\n",
      "INFO:tensorflow:loss = 73.19465, step = 1001 (4.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.2639\n",
      "INFO:tensorflow:loss = 67.73362, step = 1101 (4.299 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.1832\n",
      "INFO:tensorflow:loss = 67.38487, step = 1201 (4.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.0978\n",
      "INFO:tensorflow:loss = 69.99945, step = 1301 (4.329 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.2531\n",
      "INFO:tensorflow:loss = 66.55347, step = 1401 (4.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 21.3018\n",
      "INFO:tensorflow:loss = 69.458305, step = 1501 (4.694 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.2639\n",
      "INFO:tensorflow:loss = 67.64371, step = 1601 (4.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 22.9866\n",
      "INFO:tensorflow:loss = 61.86545, step = 1701 (4.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.2101\n",
      "INFO:tensorflow:loss = 67.01257, step = 1801 (4.307 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.194\n",
      "INFO:tensorflow:loss = 61.598545, step = 1901 (4.311 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into F:/Ania/model-builder-all\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 67.0801.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-13-23:56:02\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from F:/Ania/model-builder-all\\model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-13-23:56:32\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.74053293, accuracy_baseline = 0.61448693, auc = 0.7986872, auc_precision_recall = 0.85078675, average_loss = 0.5222881, global_step = 2000, label/mean = 0.61448693, loss = 66.84663, precision = 0.7550484, prediction/mean = 0.6210954, recall = 0.8551892\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: F:/Ania/model-builder-all\\model.ckpt-2000\n",
      "accuracy: 0.74053293\n",
      "accuracy_baseline: 0.61448693\n",
      "auc: 0.7986872\n",
      "auc_precision_recall: 0.85078675\n",
      "average_loss: 0.5222881\n",
      "global_step: 2000\n",
      "label/mean: 0.61448693\n",
      "loss: 66.84663\n",
      "precision: 0.7550484\n",
      "prediction/mean: 0.6210954\n",
      "recall: 0.8551892\n"
     ]
    }
   ],
   "source": [
    "#funckja dla treningu modelu\n",
    "def train_function(X_as_df, y_as_df):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=X_as_df,\n",
    "        y=y_as_df,\n",
    "        shuffle=True,\n",
    "    )\n",
    "#funckja dla testowania modelu\n",
    "def test_function(X_as_df,y_as_df):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=X_as_df,\n",
    "        y=y_as_df,\n",
    "        shuffle=False,\n",
    "    )\n",
    "#trening modelu\n",
    "model_trained = model.train(input_fn=train_function(X_train, y_train), steps=2000)\n",
    "#ewaluacja modelu\n",
    "results = model_trained.evaluate(input_fn=test_function(X_test, y_test))\n",
    "#wypisanie ewaluacji\n",
    "for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### OSTATECZNE WYNIKI\n",
    "\n",
    "1. accuracy: 0.74053293\n",
    "2. accuracy_baseline: 0.61448693\n",
    "3. __*auc: 0.7986872*__\n",
    "4. auc_precision_recall: 0.85078675\n",
    "5. average_loss: 0.5222881\n",
    "6. global_step: 2000\n",
    "7. label/mean: 0.61448693\n",
    "8. loss: 66.84663\n",
    "9. precision: 0.7550484\n",
    "10. prediction/mean: 0.6210954\n",
    "11. recall: 0.8551892"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Spostrzeżenia i wnioski:\n",
    "1. Bardzo ważną rzeczą są zdefiniowane feature columns. Odpowiednie dobranie ich znacząco poprawia wynik.\n",
    "2. Należy zwracać uwagę na dane. Zarówno same dane - np. ilość, jak i ich reprezentacja (np. liczbowa, boolowska czy string) mogą mieć znaczenie przy definiowaniu feature columns. \n",
    "3. Dobre dobranie klasyfikatora - należy mieć na uwadze co chcemy osiągnąć i wybrać do tego celu odpowiednie narzędzie jakim jest klasyfikator/regresor/sieć neuronowa.\n",
    "4. Należy zadbać o to, aby odpowiednio dostosować model i feature columns. Nie każdy model może przyjąć wszystkie dane.\n",
    "5. Optymalizacja - nie tylko należy dbać o optymalizację kodu, ale również modelu. Dzięki różnego rodzaju optymalizatorom oraz ich parametrom można poprawić wynik.\n",
    "6. Wszelkiego rodzaju parametry na każdym etapie projektu też mają wpływ na wynik. Należy dokładnie przeczytać dokumentację dla poszczególnych funkcji oraz dobrać odpowiednie wartości parametrów.\n",
    "7. Problemy wydajnościowe. Projekt był obliczany na CPU, ze wzlędu na brak wystarczająco dobrej karty graficznej. Pojawiały się przez to problemy wydajnościowe związane z zajetością procesora. Aby zminimalizować to ryzyko należy nie używać przeglądarki Google Chrome, która opóźnia obliczenia oraz wszelkich innych programów mających duży wpływ na procesor.\n",
    "8. Założenie początkowe __*czasem mniej może oznaczać więcej*__ zostało przetestowane i potwierdzone. Mimo małej ilości danych otrzymany wynik jest zadowalający.\n",
    "\n",
    "---\n",
    "\n",
    "#### Literatura:\n",
    "1. Dokumentacja biblioteki pandas, https://pandas.pydata.org/ [dostęp: 14.01.2019],\n",
    "2. Dokumentacja biblioteki scikit-learn, https://scikit-learn.org/stable/ [dostęp: 14.01.2019],\n",
    "3. Dokumentacja biblioteki tensorflow, https://www.tensorflow.org/ [dostęp: 14.01.2019],\n",
    "4. Forum dla programistów, https://stackoverflow.com/ [dostęp: 14.01.2019].\n",
    "\n",
    "---\n",
    "\n",
    "#### Cały kod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "path_to_file = 'F:/Ania/hetrec2011-movielens-2k-v2/'\n",
    "\n",
    "ratings = pd.read_csv(path_to_file + 'user_ratedmovies-timestamps.dat', engine = 'python', delimiter=\"\\t\", header = 0, usecols = ['userID', 'movieID', 'rating'])\n",
    "\n",
    "ratings.drop_duplicates()\n",
    "\n",
    "users = list(pd.unique(ratings['userID']))\n",
    "movies = list(pd.unique(ratings['movieID']))\n",
    "\n",
    "user_id_vocabulary = tf.feature_column.categorical_column_with_vocabulary_list('userID', users)\n",
    "movie_id_vocabulary = tf.feature_column.categorical_column_with_vocabulary_list('movieID', movies)\n",
    "user_id_indicator_vocabulary = tf.feature_column.indicator_column(user_id_vocabulary)\n",
    "movie_id_indicator_vocabulary = tf.feature_column.indicator_column(movie_id_vocabulary)\n",
    "user_id_bucketized=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('userID'), boundaries=[700, 1400])\n",
    "movie_id_bucketized=tf.feature_column.bucketized_column(tf.feature_column.numeric_column('movieID'), boundaries=[3370, 6740])\n",
    "user_id_indicator_bucketized = tf.feature_column.indicator_column(user_id_bucketized)\n",
    "movie_id_indicator_bucketized = tf.feature_column.indicator_column(movie_id_bucketized)\n",
    "\n",
    "wide_columns = [\n",
    "    tf.feature_column.crossed_column(['userID', 'movieID'], hash_bucket_size=5000)\n",
    "]\n",
    "deep_columns = [\n",
    "    user_id_indicator_bucketized,\n",
    "    movie_id_indicator_bucketized,\n",
    "    user_id_indicator_vocabulary,\n",
    "    movie_id_indicator_vocabulary\n",
    "]\n",
    "\n",
    "model_directory = 'F:/Ania/model-builder-all'\n",
    "\n",
    "model = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_directory,\n",
    "    linear_feature_columns=wide_columns,\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    linear_optimizer=tf.train.AdadeltaOptimizer(learning_rate=0.1),\n",
    "    dnn_optimizer=tf.train.AdagradOptimizer(learning_rate=0.1),\n",
    "    dnn_hidden_units=[512, 256]\n",
    ")\n",
    "\n",
    "ratings[\"rating\"] = ratings[\"rating\"] >= 3.5\n",
    "y = ratings[\"rating\"]\n",
    "X = ratings.drop(\"rating\",axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "\n",
    "def input_fn_train(X_as_df, y_as_df):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=X_as_df,\n",
    "        y=y_as_df,\n",
    "        shuffle=True,\n",
    "    )\n",
    "def tf_eval_input_fn(X_as_df,y_as_df):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=X_as_df,\n",
    "        y=y_as_df,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "model_trained = model.train(input_fn=input_fn_train(X_train, y_train), steps=2000)\n",
    "results = model_trained.evaluate(input_fn=tf_eval_input_fn(X_test, y_test))\n",
    "for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
